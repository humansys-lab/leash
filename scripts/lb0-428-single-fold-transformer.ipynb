{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Disclaimer !!!\n","\n","This is illustrative code for a high performance transformer. I use my local machine for training and inference. I did not check if this code can be run on kaggle notbook or not.\n","\n","i achieve lb0.428 (new metric) for single-fold without tricks."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import polars as pl\n","import pandas as pd\n","import os\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import numpy as np\n","import random\n","import math\n","from functools import partial\n","\n","#model ====================================\n","# https://gist.github.com/kklemon/98e491ff877c497668c715541f1bf478\n","# refer to the link above to get fast flash attention wrapper\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class Config:\n","    PREPROCESS = False\n","    KAGGLE_NOTEBOOK = False\n","    DEBUG = True\n","    \n","    SEED = 42\n","    EPOCHS = 10\n","    BATCH_SIZE = 4096\n","    LR = 1e-3\n","    WD = 1e-6\n","    PATIENCE = 10\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    NBR_FOLDS = 15\n","    SELECTED_FOLDS = [0]\n","    \n","    \n","if Config.DEBUG:\n","    n_rows = 3*10**4\n","else:\n","    n_rows = None\n","    \n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if Config.KAGGLE_NOTEBOOK:\n","    RAW_DIR = \"/kaggle/input/leash-BELKA/\"\n","    PROCESSED_DIR = \"/kaggle/input/belka-enc-dataset\"\n","    OUTPUT_DIR = \"\"\n","    MODEL_DIR = \"\"\n","else:\n","    RAW_DIR = \"../data/raw/\"\n","    PROCESSED_DIR = \"../data/processed/\"\n","    OUTPUT_DIR = \"../data/tf-dataset/\"\n","    MODEL_DIR = \"../models/\"\n","\n","TRAIN_DATA_NAME = \"train_enc.parquet\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","set_seeds(seed=Config.SEED)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-07T03:08:51.92163Z","iopub.status.busy":"2024-06-07T03:08:51.921268Z","iopub.status.idle":"2024-06-07T03:08:51.975279Z","shell.execute_reply":"2024-06-07T03:08:51.974325Z","shell.execute_reply.started":"2024-06-07T03:08:51.921601Z"},"trusted":true},"outputs":[],"source":["\n","#tokenization ====================================\n","\n","# https://www.ascii-code.com/\n","MOLECULE_DICT = {\n","    'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n","    '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25,\n","    '=': 26, '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36\n","}\n","MAX_MOLECULE_ID = np.max(list(MOLECULE_DICT.values()))\n","VOCAB_SIZE = MAX_MOLECULE_ID + 10\n","UNK = 255  # disallow: will cuase error\n","BOS = MAX_MOLECULE_ID + 1\n","EOS = MAX_MOLECULE_ID + 2\n","# rest are reserved\n","PAD = 0\n","MAX_LENGTH = 160\n","\n","MOLECULE_LUT = np.full(256, fill_value=UNK, dtype=np.uint8)\n","for k, v in MOLECULE_DICT.items():\n","    ascii = ord(k)\n","    MOLECULE_LUT[ascii] = v\n","\n","        \n","def make_token(s):\n","    \n","    t = np.frombuffer(s, np.uint8)\n","    t = MOLECULE_LUT[t]\n","    t = t.tolist()\n","\n","    L = len(t) + 2\n","    token_id = [BOS] + t + [EOS] + [PAD] * (MAX_LENGTH - L)\n","    token_mask = [1] * L + [0] * (MAX_LENGTH - L)\n","        \n","    return token_id, token_mask\n","\n","\n","#note this is byte-string!!!\n","# string to byte-string: str.encode(s)\n","smiles = b'C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1'  \n","token_id, token_mask = make_token(smiles)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# molecule_smilesのみを取得\n","train_raw = pl.read_parquet(os.path.join(RAW_DIR, \"train.parquet\"), n_rows=n_rows, columns=[\"molecule_smiles\", \"protein_name\", \"binds\"]).to_pandas()\n","smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles']\n","\n","# SMILESトークン化関数\n","def make_token(s):\n","    MOLECULE_LUT = np.full(256, fill_value=255, dtype=np.uint8)\n","    for k, v in MOLECULE_DICT.items():\n","        MOLECULE_LUT[ord(k)] = v\n","    t = np.frombuffer(s.encode(), np.uint8)\n","    t = MOLECULE_LUT[t]\n","    t = t.tolist()\n","    L = len(t) + 2\n","    token_id = [37] + t + [38] + [0] * (160 - L)\n","    token_mask = [1] * L + [0] * (160 - L)\n","    return token_id, token_mask\n","\n","# すべてのSMILESをトークン化\n","tokens = smiles.apply(make_token)\n","\n","# トークン列とマスク列をデータフレームに変換\n","def expand_tokens_to_df(tokens, max_length):\n","    token_columns = {f\"enc{i}\": [] for i in range(max_length)}\n","    mask_columns = {f\"enc{i}\": [] for i in range(max_length)}\n","\n","    for token_id, mask in tokens:\n","        for i in range(max_length):\n","            token_columns[f\"enc{i}\"].append(token_id[i] if i < len(token_id) else None)\n","            mask_columns[f\"enc{i}\"].append(mask[i] if i < len(mask) else None)\n","\n","    token_df = pd.DataFrame(token_columns)\n","    mask_df = pd.DataFrame(mask_columns)\n","\n","    return token_df, mask_df\n","\n","train, mask_df = expand_tokens_to_df(tokens, 160)\n","train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n","train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n","train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# save\n","train.to_parquet(os.path.join(OUTPUT_DIR, \"train_enc.parquet\"))\n","mask_df.to_parquet(os.path.join(OUTPUT_DIR, \"mask.parquet\"))"]},{"cell_type":"markdown","metadata":{},"source":["for modeling, you need flash attnetion + torch compile to make it run fast.\n","I train on A6000 gpu with batch size= 2000."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:51.977563Z","iopub.status.busy":"2024-06-07T03:08:51.97725Z","iopub.status.idle":"2024-06-07T03:08:51.988189Z","shell.execute_reply":"2024-06-07T03:08:51.987025Z","shell.execute_reply.started":"2024-06-07T03:08:51.977537Z"},"trusted":true},"outputs":[],"source":["\n","\n","class FlashAttentionTransformerEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model,\n","        num_layers,\n","        num_heads=None,\n","        dim_feedforward=None,\n","        dropout=0.0,\n","        norm_first=False,\n","        activation=F.gelu,\n","        rotary_emb_dim=0,\n","    ):\n","        super().__init__()\n","\n","        try:\n","            from flash_attn.bert_padding import pad_input, unpad_input\n","            from flash_attn.modules.block import Block\n","            from flash_attn.modules.mha import MHA\n","            from flash_attn.modules.mlp import Mlp\n","        except ImportError:\n","            raise ImportError('Please install flash_attn from https://github.com/Dao-AILab/flash-attention')\n","        \n","        self._pad_input = pad_input\n","        self._unpad_input = unpad_input\n","\n","        if num_heads is None:\n","            num_heads = dim_model // 64\n","        \n","        if dim_feedforward is None:\n","            dim_feedforward = dim_model * 4\n","\n","        if isinstance(activation, str):\n","            activation = {\n","                'relu': F.relu,\n","                'gelu': F.gelu\n","            }.get(activation)\n","\n","            if activation is None:\n","                raise ValueError(f'Unknown activation {activation}')\n","\n","        mixer_cls = partial(\n","            MHA,\n","            num_heads=num_heads,\n","            use_flash_attn=True,\n","            rotary_emb_dim=rotary_emb_dim\n","        )\n","\n","        mlp_cls = partial(Mlp, hidden_features=dim_feedforward)\n","\n","        self.layers = nn.ModuleList([\n","            Block(\n","                dim_model,\n","                mixer_cls=mixer_cls,\n","                mlp_cls=mlp_cls,\n","                resid_dropout1=dropout,\n","                resid_dropout2=dropout,\n","                prenorm=norm_first,\n","            ) for _ in range(num_layers)\n","        ])\n","    \n","    def forward(self, x, src_key_padding_mask=None):\n","        batch, seqlen = x.shape[:2]\n","\n","        if src_key_padding_mask is None:\n","            for layer in self.layers:\n","                x = layer(x)\n","        else:\n","            x, indices, cu_seqlens, max_seqlen_in_batch = self._unpad_input(x, ~src_key_padding_mask)\n","            \n","            for layer in self.layers:\n","                x = layer(x, mixer_kwargs=dict(\n","                    cu_seqlens=cu_seqlens,\n","                    max_seqlen=max_seqlen_in_batch\n","                ))\n","      \n","\n","            x = self._pad_input(x, indices, batch, seqlen)\n","            \n","        return x\n","\n","class Conv1dBnRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, is_bn, **kwargs):\n","        super(Conv1dBnRelu, self).__init__()\n","        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n","        self.is_bn = is_bn\n","        if self.is_bn:\n","            self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.is_bn:\n","            x = self.bn(x)\n","        return self.relu(x)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=256):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[ :,:x.size(1)]\n","        \n","        return x\n","\n","class Net(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim=512\n","\n","        self.output_type = ['infer', 'loss']\n","        self.pe = PositionalEncoding(embed_dim,max_len=256)\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3,stride=1,padding=1, is_bn=True),\n","        )  #just a simple conv1d-bn-relu . for bn use: BN = partial(nn.BatchNorm1d, eps=5e-3,momentum=0.1)\n","\n","        self.tx_encoder = FlashAttentionTransformerEncoder(\n","            dim_model=embed_dim,\n","            num_heads=8,\n","            dim_feedforward=embed_dim*4,\n","            dropout=0.1,\n","            norm_first=False,\n","            activation=F.gelu,\n","            rotary_emb_dim=0,\n","            num_layers=7,\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","\n","    def forward(self, batch):\n","        smiles_token_id   = batch['smiles_token_id'].long()\n","        smiles_token_mask = batch['smiles_token_mask'].long()\n","        B, L = smiles_token_id.shape\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","\n","        x = self.pe(x)\n","        z = self.tx_encoder(\n","            x=x,\n","            src_key_padding_mask=smiles_token_mask==0,\n","        )\n","\n","\n","        m = smiles_token_mask.unsqueeze(-1).float()\n","        pool = (z*m).sum(1)/m.sum(1)\n","        bind = self.bind(pool)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output\n","\n","    \n","#-------------------------------------\n","#dummy code to check net\n","def run_check_net():\n","    max_length = MAX_LENGTH\n","    batch_size = 500\n","\n","    batch = {\n","        'smiles_token_id': torch.from_numpy(np.random.choice(VOCAB_SIZE, (batch_size, max_length))).byte().cuda(),\n","        'smiles_token_mask': torch.from_numpy(np.random.choice(2, (batch_size, max_length))).byte().cuda(),\n","        'bind': torch.from_numpy(np.random.choice(2, (batch_size, 3))).float().cuda(),\n","    }\n","     \n","    net = Net().cuda()\n","    #print(net)\n","    #net.train()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n","            output = net(batch)\n","\n","    # ---\n","    print('batch')\n","    for k, v in batch.items():\n","        if k=='idx':\n","            print(f'{k:>32} : {len(v)} ')\n","        else:\n","            print(f'{k:>32} : {v.shape} ')\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print(f'{k:>32} : {v.shape} ')\n","    print('loss')\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print(f'{k:>32} : {v.item()} ')\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["FEATURES = [f'enc{i}' for i in range(160)]\n","TARGETS = ['bind1', 'bind2', 'bind3']\n","\n","# バッチに分ける\n","def get_batch(df, mask_df, batch_size=32):\n","    for i in range(0, len(df), batch_size):\n","        batch = {}\n","        batch['smiles_token_id'] = torch.from_numpy(df[FEATURES].values[i:i+batch_size]).byte().cuda()\n","        batch['smiles_token_mask'] = torch.from_numpy(mask_df[FEATURES].values[i:i+batch_size]).byte().cuda()\n","        batch['bind'] = torch.from_numpy(df[TARGETS].values[i:i+batch_size]).float().cuda()\n","        yield batch\n","    \n","batch = get_batch(train, mask_df, batch_size=2048)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'Net' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iter:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_train\u001b[39m():\n\u001b[0;32m---> 17\u001b[0m     net \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     18\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"]}],"source":["\n","# 学習\n","def run_train():\n","    net = Net().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(10):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            output = net(batch)\n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","    \n","run_train()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["batch\n","                 smiles_token_id : torch.Size([10000, 160]) \n","               smiles_token_mask : torch.Size([10000, 160]) \n","                            bind : torch.Size([10000, 3]) \n","output\n","                            bind : torch.Size([10000, 3]) \n","loss\n","                        bce_loss : 0.5559630393981934 \n"]}],"source":["def check_net():\n","    max_length = MAX_LENGTH\n","\n","\n","    batch = {\n","        'smiles_token_id': torch.from_numpy(train[FEATURES].values).byte().cuda(),\n","        'smiles_token_mask': torch.from_numpy(mask_df.values).byte().cuda(),\n","        'bind': torch.from_numpy(train[TARGETS].values).float().cuda(),\n","    }\n","     \n","    net = Net().cuda()\n","    #print(net)\n","    #net.train()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n","            output = net(batch)\n","\n","    # ---\n","    print('batch')\n","    for k, v in batch.items():\n","        if k=='idx':\n","            print(f'{k:>32} : {len(v)} ')\n","        else:\n","            print(f'{k:>32} : {v.shape} ')\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print(f'{k:>32} : {v.shape} ')\n","    print('loss')\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print(f'{k:>32} : {v.item()} ')\n","\n","\n","check_net()\n"]},{"cell_type":"markdown","metadata":{},"source":["I also tried other next generation seq models.  \n","e.g. mamba, xlstm, Griffin (deepmind's attention RNN)  \n","\n","I am looking for a faster alternative. But so far transformer + flash attnetion2 is still the fastest  (for small dim and num of layers). for performance, i think these models are smiliar."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:51.990281Z","iopub.status.busy":"2024-06-07T03:08:51.989893Z","iopub.status.idle":"2024-06-07T03:08:52.003495Z","shell.execute_reply":"2024-06-07T03:08:52.002567Z","shell.execute_reply.started":"2024-06-07T03:08:51.99025Z"},"trusted":true},"outputs":[],"source":["\n","\n","#xlstm model\n","# offical repo: https://github.com/NX-AI/xlstm\n","from xlstm import (\n","    xLSTMBlockStack,\n","    xLSTMBlockStackConfig,\n","    mLSTMBlockConfig,\n","    mLSTMLayerConfig,\n","    sLSTMBlockConfig,\n","    sLSTMLayerConfig,\n","    FeedForwardConfig,\n",")\n","\n","class Xlstm(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim = 256\n","\n","        self.output_type = ['infer', 'loss']\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=0)\n","\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3, stride=1, padding=1, is_bn=False),\n","        )\n","\n","        self.lstm_encoder = xLSTMBlockStack(\n","            config = xLSTMBlockStackConfig(\n","                mlstm_block=mLSTMBlockConfig(\n","                    mlstm=mLSTMLayerConfig(\n","                        conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n","                    )\n","                ),\n","                slstm_block=sLSTMBlockConfig(\n","                    slstm=sLSTMLayerConfig(\n","                        backend='cuda',\n","                        batch_size=64,\n","                        num_heads=8,\n","                        conv1d_kernel_size=4,\n","                        bias_init='powerlaw_blockdependent',\n","                    ),\n","                    feedforward=FeedForwardConfig(proj_factor=1.3, act_fn='gelu'),\n","                ),\n","                context_length=MAX_LENGTH,\n","                num_blocks=6,\n","                embedding_dim=embed_dim,\n","                slstm_at=[1],\n","            )\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","\n","    def forward(self, batch):\n","        smiles_token_id = batch['smiles_token_id'].long()\n","        B, L = smiles_token_id.shape\n","\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","\n","        x = self.lstm_encoder(x)\n","        last = x.mean(1)\n","        bind = self.bind(last)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["\n","# 学習\n","def run_train():\n","    net = Xlstm().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(10):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            output = net(batch)\n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","\n","run_train()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:52.006055Z","iopub.status.busy":"2024-06-07T03:08:52.005646Z","iopub.status.idle":"2024-06-07T03:08:52.020395Z","shell.execute_reply":"2024-06-07T03:08:52.019248Z","shell.execute_reply.started":"2024-06-07T03:08:52.00601Z"},"trusted":true},"outputs":[],"source":["\n","# official repo: https://github.com/state-spaces/mamba\n","\n","#https://github.com/state-spaces/mamba/issues/355\n","# there is a bug? i cannot try mamba2\n","\n","from mamba_ssm import Mamba\n","from torch.nn.init import xavier_uniform_\n","from functools import partial\n","\n","\n","# パラメータの初期化\n","def _init_weights(module, n_layer, n_residuals_per_layer):\n","    if isinstance(module, (nn.Linear, nn.Conv1d)):\n","        xavier_uniform_(module.weight)\n","    if isinstance(module, nn.BatchNorm1d):\n","        module.weight.data.fill_(1)\n","        module.bias.data.zero_()\n","\n","# もし RMSNorm が必要な場合、その実装を追加してください\n","class RMSNorm(nn.Module):\n","    def __init__(self, d_model, eps=1e-8):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.ones(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.pow(2).mean(-1, keepdim=True)\n","        normed = x * self.scale / torch.sqrt(mean + self.eps)\n","        return normed\n","\n","class MambaNet(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim=256\n","        num_layer=6\n","        self.output_type = ['infer', 'loss']\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n","        self.pe = PositionalEncoding(embed_dim,max_len=256)\n","\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3,stride=1,padding=1, is_bn=True),\n","        )\n","\n","        self.mamba_encoder = Mamba(\n","            d_model=128, \n","            d_state=16,\n","            d_conv=4,\n","            expand=2,\n","        )\n","        \n","\n","        self.norm_f = nn.LayerNorm ( #RMSNorm\n","            embed_dim, eps=1e-4\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","        self.apply(\n","            partial(\n","                _init_weights,\n","                n_layer=num_layer,\n","                n_residuals_per_layer=2,\n","            )\n","        )\n","\n","    def forward(self, batch):\n","        smiles_token_id = batch['smiles_token_id'].long()\n","        smiles_token_mask = batch['smiles_token_mask'].long()\n","        B, L  = smiles_token_id.shape\n","\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","      \n","        hidden, residual = x, None\n","        for mamba in self.mamba_encoder:\n","            hidden, residual = mamba(\n","                hidden, residual, inference_params=None\n","            )\n","            hidden = F.dropout(hidden,p=0.1, training=self.training)\n","\n","        #z=hidden\n","        z = layer_norm_fn(\n","            hidden,\n","            self.norm_f.weight,\n","            self.norm_f.bias,\n","            eps=self.norm_f.eps,\n","            residual=residual,\n","            prenorm=False,\n","            residual_in_fp32=False,\n","            is_rms_norm=isinstance(self.norm_f, RMSNorm)\n","        )\n","\n","        #pool = z.mean(1)\n","        m = smiles_token_mask.unsqueeze(2).float()\n","        pool = (z*m).sum(1)/m.sum(1)\n","        bind = self.bind(pool)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'Mamba' object is not iterable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iter:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_batch(train, mask_df, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)):\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbce_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[25], line 80\u001b[0m, in \u001b[0;36mMambaNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     79\u001b[0m hidden, residual \u001b[38;5;241m=\u001b[39m x, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mamba \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_encoder:\n\u001b[1;32m     81\u001b[0m     hidden, residual \u001b[38;5;241m=\u001b[39m mamba(\n\u001b[1;32m     82\u001b[0m         hidden, residual, inference_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(hidden,p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n","\u001b[0;31mTypeError\u001b[0m: 'Mamba' object is not iterable"]}],"source":["# 学習\n","def run_train():\n","    net = MambaNet().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(10):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            output = net(batch)\n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","\n","run_train()"]},{"cell_type":"markdown","metadata":{},"source":["#### "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8006601,"sourceId":67356,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
