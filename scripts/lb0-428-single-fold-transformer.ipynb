{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Disclaimer !!!\n","\n","This is illustrative code for a high performance transformer. I use my local machine for training and inference. I did not check if this code can be run on kaggle notbook or not.\n","\n","i achieve lb0.428 (new metric) for single-fold without tricks."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import polars as pl\n","import pandas as pd\n","import os\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import numpy as np\n","import random\n","import math\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor\n","\n","#model ====================================\n","# https://gist.github.com/kklemon/98e491ff877c497668c715541f1bf478\n","# refer to the link above to get fast flash attention wrapper\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class Config:\n","    PREPROCESS = False\n","    KAGGLE_NOTEBOOK = False\n","    DEBUG = True\n","    \n","    SEED = 42\n","    EPOCHS = 10\n","    BATCH_SIZE = 4096\n","    LR = 1e-3\n","    WD = 1e-6\n","    PATIENCE = 10\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    NBR_FOLDS = 15\n","    SELECTED_FOLDS = [0]\n","    \n","    \n","if Config.DEBUG:\n","    n_rows = 3*10**4\n","else:\n","    n_rows = None\n","    \n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if Config.KAGGLE_NOTEBOOK:\n","    RAW_DIR = \"/kaggle/input/leash-BELKA/\"\n","    PROCESSED_DIR = \"/kaggle/input/belka-enc-dataset\"\n","    OUTPUT_DIR = \"\"\n","    MODEL_DIR = \"\"\n","else:\n","    RAW_DIR = \"../data/raw/\"\n","    PROCESSED_DIR = \"../data/processed/\"\n","    OUTPUT_DIR = \"../data/tf-dataset-original/\"\n","    MODEL_DIR = \"../models/\"\n","\n","TRAIN_DATA_NAME = \"train_enc.parquet\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","set_seeds(seed=Config.SEED)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-07T03:08:51.92163Z","iopub.status.busy":"2024-06-07T03:08:51.921268Z","iopub.status.idle":"2024-06-07T03:08:51.975279Z","shell.execute_reply":"2024-06-07T03:08:51.974325Z","shell.execute_reply.started":"2024-06-07T03:08:51.921601Z"},"trusted":true},"outputs":[],"source":["\n","#tokenization ====================================\n","\n","# https://www.ascii-code.com/\n","MOLECULE_DICT = {\n","    'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n","    '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25,\n","    '=': 26, '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36\n","}\n","MAX_MOLECULE_ID = np.max(list(MOLECULE_DICT.values()))\n","VOCAB_SIZE = MAX_MOLECULE_ID + 10\n","UNK = 255  # disallow: will cuase error\n","BOS = MAX_MOLECULE_ID + 1\n","EOS = MAX_MOLECULE_ID + 2\n","# rest are reserved\n","PAD = 0\n","MAX_LENGTH = 160\n","\n","MOLECULE_LUT = np.full(256, fill_value=UNK, dtype=np.uint8)\n","for k, v in MOLECULE_DICT.items():\n","    ascii = ord(k)\n","    MOLECULE_LUT[ascii] = v\n","\n","\n","# SMILESトークン化関数\n","def make_token(s):\n","    MOLECULE_LUT = np.full(256, fill_value=255, dtype=np.uint8)\n","    for k, v in MOLECULE_DICT.items():\n","        MOLECULE_LUT[ord(k)] = v\n","    t = np.frombuffer(s.encode(), np.uint8)\n","    t = MOLECULE_LUT[t]\n","    t = t.tolist()\n","    L = len(t) + 2\n","    token_id = [37] + t + [38] + [0] * (160 - L)\n","    token_mask = [1] * L + [0] * (160 - L)\n","    return token_id, token_mask\n","\n","\n","# トークン列とマスク列をデータフレームに変換\n","def expand_tokens_to_df(tokens, max_length):\n","    token_columns = {f\"enc{i}\": [] for i in range(max_length)}\n","    mask_columns = {f\"enc{i}\": [] for i in range(max_length)}\n","\n","    for token_id, mask in tokens:\n","        for i in range(max_length):\n","            token_columns[f\"enc{i}\"].append(token_id[i] if i < len(token_id) else None)\n","            mask_columns[f\"enc{i}\"].append(mask[i] if i < len(mask) else None)\n","\n","    token_df = pd.DataFrame(token_columns)\n","    mask_df = pd.DataFrame(mask_columns)\n","\n","    return token_df, mask_df"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["data loaded ../data/shuffled-dataset/train_0.parquet (30000, 4)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m smiles \u001b[38;5;241m=\u001b[39m train_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmolecule_smiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m smiles\u001b[38;5;241m.\u001b[39mapply(make_token)\n\u001b[0;32m----> 7\u001b[0m train, mask_df \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_tokens_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbind1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbind1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbind2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbind2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mexpand_tokens_to_df\u001b[0;34m(tokens, max_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m         token_columns[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(token_id[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_id) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m         mask_columns[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(mask[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 48\u001b[0m token_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m mask_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(mask_columns)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_df, mask_df\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/internals/construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/internals/construction.py:629\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    626\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[1;32m    627\u001b[0m     val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m--> 629\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[1;32m    631\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n","File \u001b[0;32m~/miniconda3/envs/leash/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:139\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m    138\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[0;32m--> 139\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(10):\n","    input_path = os.path.join(\"../data/shuffled-dataset/\", f\"train_{i}.parquet\")\n","    train_raw = pl.read_parquet(input_path, n_rows=n_rows, columns=[\"molecule_smiles\", \"bind1\", \"bind2\", \"bind3\"]).to_pandas()\n","    print(\"data loaded\", input_path, train_raw.shape)\n","    smiles = train_raw[\"molecule_smiles\"]\n","    tokens = smiles.apply(make_token)\n","    train, mask_df = expand_tokens_to_df(tokens, 160)\n","    train[\"bind1\"] = train_raw[\"bind1\"]\n","    train[\"bind2\"] = train_raw[\"bind2\"]\n","    train[\"bind3\"] = train_raw[\"bind3\"]\n","\n","    # save\n","    path = os.path.join(OUTPUT_DIR, f\"train_enc_{i}.parquet\")\n","    # train.to_parquet(path)\n","    # mask_df.to_parquet(os.path.join(OUTPUT_DIR, f\"train_mask_{i}.parquet\"))\n","    print(\"data saved\", path)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (100_000, 163)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>enc0</th><th>enc1</th><th>enc2</th><th>enc3</th><th>enc4</th><th>enc5</th><th>enc6</th><th>enc7</th><th>enc8</th><th>enc9</th><th>enc10</th><th>enc11</th><th>enc12</th><th>enc13</th><th>enc14</th><th>enc15</th><th>enc16</th><th>enc17</th><th>enc18</th><th>enc19</th><th>enc20</th><th>enc21</th><th>enc22</th><th>enc23</th><th>enc24</th><th>enc25</th><th>enc26</th><th>enc27</th><th>enc28</th><th>enc29</th><th>enc30</th><th>enc31</th><th>enc32</th><th>enc33</th><th>enc34</th><th>enc35</th><th>enc36</th><th>&hellip;</th><th>enc126</th><th>enc127</th><th>enc128</th><th>enc129</th><th>enc130</th><th>enc131</th><th>enc132</th><th>enc133</th><th>enc134</th><th>enc135</th><th>enc136</th><th>enc137</th><th>enc138</th><th>enc139</th><th>enc140</th><th>enc141</th><th>enc142</th><th>enc143</th><th>enc144</th><th>enc145</th><th>enc146</th><th>enc147</th><th>enc148</th><th>enc149</th><th>enc150</th><th>enc151</th><th>enc152</th><th>enc153</th><th>enc154</th><th>enc155</th><th>enc156</th><th>enc157</th><th>enc158</th><th>enc159</th><th>bind1</th><th>bind2</th><th>bind3</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>37</td><td>28</td><td>26</td><td>8</td><td>17</td><td>33</td><td>29</td><td>30</td><td>2</td><td>32</td><td>19</td><td>29</td><td>8</td><td>3</td><td>3</td><td>5</td><td>32</td><td>17</td><td>8</td><td>12</td><td>27</td><td>12</td><td>12</td><td>12</td><td>17</td><td>7</td><td>19</td><td>12</td><td>17</td><td>7</td><td>19</td><td>12</td><td>27</td><td>19</td><td>33</td><td>12</td><td>27</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>26</td><td>8</td><td>8</td><td>29</td><td>8</td><td>3</td><td>5</td><td>32</td><td>17</td><td>33</td><td>12</td><td>27</td><td>35</td><td>12</td><td>17</td><td>33</td><td>8</td><td>12</td><td>18</td><td>12</td><td>12</td><td>12</td><td>35</td><td>12</td><td>18</td><td>36</td><td>35</td><td>18</td><td>12</td><td>12</td><td>35</td><td>12</td><td>18</td><td>8</td><td>19</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>6</td><td>17</td><td>26</td><td>28</td><td>19</td><td>17</td><td>26</td><td>28</td><td>19</td><td>33</td><td>8</td><td>27</td><td>8</td><td>8</td><td>8</td><td>8</td><td>27</td><td>8</td><td>33</td><td>12</td><td>27</td><td>35</td><td>12</td><td>17</td><td>33</td><td>8</td><td>8</td><td>18</td><td>17</td><td>28</td><td>19</td><td>8</td><td>8</td><td>6</td><td>8</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>28</td><td>26</td><td>8</td><td>17</td><td>33</td><td>29</td><td>30</td><td>2</td><td>32</td><td>19</td><td>29</td><td>8</td><td>3</td><td>3</td><td>5</td><td>32</td><td>17</td><td>8</td><td>12</td><td>27</td><td>12</td><td>12</td><td>12</td><td>17</td><td>8</td><td>1</td><td>19</td><td>12</td><td>17</td><td>8</td><td>1</td><td>19</td><td>12</td><td>27</td><td>19</td><td>33</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>12</td><td>27</td><td>12</td><td>12</td><td>17</td><td>33</td><td>12</td><td>18</td><td>35</td><td>12</td><td>17</td><td>33</td><td>8</td><td>29</td><td>8</td><td>3</td><td>3</td><td>5</td><td>32</td><td>4</td><td>28</td><td>8</td><td>8</td><td>33</td><td>17</td><td>8</td><td>19</td><td>29</td><td>8</td><td>3</td><td>5</td><td>32</td><td>4</td><td>12</td><td>4</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>37</td><td>28</td><td>26</td><td>8</td><td>17</td><td>33</td><td>29</td><td>30</td><td>2</td><td>32</td><td>19</td><td>29</td><td>8</td><td>3</td><td>5</td><td>32</td><td>17</td><td>8</td><td>8</td><td>8</td><td>27</td><td>8</td><td>8</td><td>8</td><td>8</td><td>8</td><td>27</td><td>19</td><td>33</td><td>12</td><td>27</td><td>35</td><td>12</td><td>17</td><td>33</td><td>12</td><td>18</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>28</td><td>12</td><td>27</td><td>12</td><td>12</td><td>12</td><td>12</td><td>17</td><td>36</td><td>12</td><td>18</td><td>12</td><td>12</td><td>17</td><td>33</td><td>12</td><td>4</td><td>35</td><td>12</td><td>17</td><td>33</td><td>8</td><td>8</td><td>12</td><td>25</td><td>12</td><td>12</td><td>35</td><td>13</td><td>25</td><td>19</td><td>35</td><td>12</td><td>17</td><td>33</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>28</td><td>8</td><td>17</td><td>26</td><td>28</td><td>19</td><td>12</td><td>27</td><td>12</td><td>12</td><td>17</td><td>33</td><td>12</td><td>18</td><td>35</td><td>12</td><td>17</td><td>33</td><td>12</td><td>4</td><td>12</td><td>35</td><td>12</td><td>17</td><td>8</td><td>1</td><td>19</td><td>12</td><td>12</td><td>4</td><td>8</td><td>17</td><td>26</td><td>28</td><td>19</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>8</td><td>29</td><td>8</td><td>3</td><td>3</td><td>5</td><td>32</td><td>17</td><td>28</td><td>8</td><td>12</td><td>27</td><td>12</td><td>12</td><td>12</td><td>12</td><td>12</td><td>27</td><td>19</td><td>29</td><td>8</td><td>3</td><td>5</td><td>32</td><td>17</td><td>33</td><td>12</td><td>27</td><td>35</td><td>12</td><td>17</td><td>33</td><td>8</td><td>8</td><td>12</td><td>18</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>37</td><td>33</td><td>22</td><td>8</td><td>12</td><td>27</td><td>12</td><td>12</td><td>12</td><td>35</td><td>12</td><td>27</td><td>33</td><td>12</td><td>27</td><td>35</td><td>12</td><td>17</td><td>33</td><td>12</td><td>18</td><td>12</td><td>12</td><td>12</td><td>17</td><td>33</td><td>4</td><td>8</td><td>8</td><td>28</td><td>8</td><td>8</td><td>4</td><td>19</td><td>12</td><td>35</td><td>18</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"],"text/plain":["shape: (100_000, 163)\n","┌──────┬──────┬──────┬──────┬───┬────────┬───────┬───────┬───────┐\n","│ enc0 ┆ enc1 ┆ enc2 ┆ enc3 ┆ … ┆ enc159 ┆ bind1 ┆ bind2 ┆ bind3 │\n","│ ---  ┆ ---  ┆ ---  ┆ ---  ┆   ┆ ---    ┆ ---   ┆ ---   ┆ ---   │\n","│ i64  ┆ i64  ┆ i64  ┆ i64  ┆   ┆ i64    ┆ i64   ┆ i64   ┆ i64   │\n","╞══════╪══════╪══════╪══════╪═══╪════════╪═══════╪═══════╪═══════╡\n","│ 37   ┆ 28   ┆ 26   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 26   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 6    ┆ 17   ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 28   ┆ 26   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 12   ┆ 27   ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ …    ┆ …    ┆ …    ┆ …    ┆ … ┆ …      ┆ …     ┆ …     ┆ …     │\n","│ 37   ┆ 28   ┆ 26   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 28   ┆ 12   ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 28   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 8    ┆ 29   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","│ 37   ┆ 33   ┆ 22   ┆ 8    ┆ … ┆ 0      ┆ 0     ┆ 0     ┆ 0     │\n","└──────┴──────┴──────┴──────┴───┴────────┴───────┴───────┴───────┘"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["train = pl.read_parquet(os.path.join(OUTPUT_DIR, \"train_enc_0.parquet\"))\n","train_mask = pl.read_parquet(os.path.join(OUTPUT_DIR, \"train_mask_0.parquet\"))\n","\n","train"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>enc0</th>\n","      <th>enc1</th>\n","      <th>enc2</th>\n","      <th>enc3</th>\n","      <th>enc4</th>\n","      <th>enc5</th>\n","      <th>enc6</th>\n","      <th>enc7</th>\n","      <th>enc8</th>\n","      <th>enc9</th>\n","      <th>...</th>\n","      <th>enc153</th>\n","      <th>enc154</th>\n","      <th>enc155</th>\n","      <th>enc156</th>\n","      <th>enc157</th>\n","      <th>enc158</th>\n","      <th>enc159</th>\n","      <th>bind1</th>\n","      <th>bind2</th>\n","      <th>bind3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>27</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>27</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>27</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>27</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>28</td>\n","      <td>12</td>\n","      <td>27</td>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99995</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99996</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99997</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99998</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99999</th>\n","      <td>37</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100000 rows × 163 columns</p>\n","</div>"],"text/plain":["       enc0  enc1  enc2  enc3  enc4  enc5  enc6  enc7  enc8  enc9  ...  \\\n","0        37     8    22     8     8    28    12    27    12    12  ...   \n","1        37     8    22     8     8    28    12    27    12    12  ...   \n","2        37     8    22     8     8    28    12    27    12    12  ...   \n","3        37     8    22     8     8    28    12    27    12    12  ...   \n","4        37     8    22     8     8    28    12    27    12    12  ...   \n","...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n","99995    37     8    22     8     8    29     8     3     3     5  ...   \n","99996    37     8    22     8     8    29     8     3     3     5  ...   \n","99997    37     8    22     8     8    29     8     3     3     5  ...   \n","99998    37     8    22     8     8    29     8     3     3     5  ...   \n","99999    37     8    22     8     8    29     8     3     3     5  ...   \n","\n","       enc153  enc154  enc155  enc156  enc157  enc158  enc159  bind1  bind2  \\\n","0           0       0       0       0       0       0       0      0      0   \n","1           0       0       0       0       0       0       0      0      0   \n","2           0       0       0       0       0       0       0      0      0   \n","3           0       0       0       0       0       0       0      0      0   \n","4           0       0       0       0       0       0       0      0      0   \n","...       ...     ...     ...     ...     ...     ...     ...    ...    ...   \n","99995       0       0       0       0       0       0       0      0      0   \n","99996       0       0       0       0       0       0       0      0      0   \n","99997       0       0       0       0       0       0       0      0      0   \n","99998       0       0       0       0       0       0       0      0      0   \n","99999       0       0       0       0       0       0       0      0      0   \n","\n","       bind3  \n","0          0  \n","1          0  \n","2          0  \n","3          0  \n","4          0  \n","...      ...  \n","99995      0  \n","99996      0  \n","99997      0  \n","99998      0  \n","99999      0  \n","\n","[100000 rows x 163 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"markdown","metadata":{},"source":["for modeling, you need flash attnetion + torch compile to make it run fast.\n","I train on A6000 gpu with batch size= 2000."]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:51.977563Z","iopub.status.busy":"2024-06-07T03:08:51.97725Z","iopub.status.idle":"2024-06-07T03:08:51.988189Z","shell.execute_reply":"2024-06-07T03:08:51.987025Z","shell.execute_reply.started":"2024-06-07T03:08:51.977537Z"},"trusted":true},"outputs":[],"source":["\n","\n","class FlashAttentionTransformerEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model,\n","        num_layers,\n","        num_heads=None,\n","        dim_feedforward=None,\n","        dropout=0.0,\n","        norm_first=False,\n","        activation=F.gelu,\n","        rotary_emb_dim=0,\n","    ):\n","        super().__init__()\n","\n","        try:\n","            from flash_attn.bert_padding import pad_input, unpad_input\n","            from flash_attn.modules.block import Block\n","            from flash_attn.modules.mha import MHA\n","            from flash_attn.modules.mlp import Mlp\n","        except ImportError:\n","            raise ImportError('Please install flash_attn from https://github.com/Dao-AILab/flash-attention')\n","        \n","        self._pad_input = pad_input\n","        self._unpad_input = unpad_input\n","\n","        if num_heads is None:\n","            num_heads = dim_model // 64\n","        \n","        if dim_feedforward is None:\n","            dim_feedforward = dim_model * 4\n","\n","        if isinstance(activation, str):\n","            activation = {\n","                'relu': F.relu,\n","                'gelu': F.gelu\n","            }.get(activation)\n","\n","            if activation is None:\n","                raise ValueError(f'Unknown activation {activation}')\n","\n","        mixer_cls = partial(\n","            MHA,\n","            num_heads=num_heads,\n","            use_flash_attn=True,\n","            rotary_emb_dim=rotary_emb_dim\n","        )\n","\n","        mlp_cls = partial(Mlp, hidden_features=dim_feedforward)\n","\n","        self.layers = nn.ModuleList([\n","            Block(\n","                dim_model,\n","                mixer_cls=mixer_cls,\n","                mlp_cls=mlp_cls,\n","                resid_dropout1=dropout,\n","                resid_dropout2=dropout,\n","                prenorm=norm_first,\n","            ) for _ in range(num_layers)\n","        ])\n","    \n","    def forward(self, x, src_key_padding_mask=None):\n","        batch, seqlen = x.shape[:2]\n","\n","        if src_key_padding_mask is None:\n","            for layer in self.layers:\n","                x = layer(x)\n","        else:\n","            x, indices, cu_seqlens, max_seqlen_in_batch = self._unpad_input(x, ~src_key_padding_mask)\n","            \n","            for layer in self.layers:\n","                x = layer(x, mixer_kwargs=dict(\n","                    cu_seqlens=cu_seqlens,\n","                    max_seqlen=max_seqlen_in_batch\n","                ))\n","      \n","\n","            x = self._pad_input(x, indices, batch, seqlen)\n","            \n","        return x\n","\n","class Conv1dBnRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, is_bn, **kwargs):\n","        super(Conv1dBnRelu, self).__init__()\n","        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n","        self.is_bn = is_bn\n","        if self.is_bn:\n","            self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.is_bn:\n","            x = self.bn(x)\n","        return self.relu(x)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=256):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[ :,:x.size(1)]\n","        \n","        return x\n","\n","class Net(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim=512\n","\n","        self.output_type = ['infer', 'loss']\n","        self.pe = PositionalEncoding(embed_dim,max_len=256)\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3,stride=1,padding=1, is_bn=True),\n","        )  #just a simple conv1d-bn-relu . for bn use: BN = partial(nn.BatchNorm1d, eps=5e-3,momentum=0.1)\n","\n","        self.tx_encoder = FlashAttentionTransformerEncoder(\n","            dim_model=embed_dim,\n","            num_heads=8,\n","            dim_feedforward=embed_dim*4,\n","            dropout=0.1,\n","            norm_first=False,\n","            activation=F.gelu,\n","            rotary_emb_dim=0,\n","            num_layers=7,\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","\n","    def forward(self, batch):\n","        smiles_token_id   = batch['smiles_token_id'].long()\n","        smiles_token_mask = batch['smiles_token_mask'].long()\n","        B, L = smiles_token_id.shape\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","\n","        x = self.pe(x)\n","        z = self.tx_encoder(\n","            x=x,\n","            src_key_padding_mask=smiles_token_mask==0,\n","        )\n","\n","\n","        m = smiles_token_mask.unsqueeze(-1).float()\n","        pool = (z*m).sum(1)/m.sum(1)\n","        bind = self.bind(pool)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output\n","\n","    \n","#-------------------------------------\n","#dummy code to check net\n","def run_check_net():\n","    max_length = MAX_LENGTH\n","    batch_size = 500\n","\n","    batch = {\n","        'smiles_token_id': torch.from_numpy(np.random.choice(VOCAB_SIZE, (batch_size, max_length))).byte().cuda(),\n","        'smiles_token_mask': torch.from_numpy(np.random.choice(2, (batch_size, max_length))).byte().cuda(),\n","        'bind': torch.from_numpy(np.random.choice(2, (batch_size, 3))).float().cuda(),\n","    }\n","     \n","    net = Net().cuda()\n","    #print(net)\n","    #net.train()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n","            output = net(batch)\n","\n","    # ---\n","    print('batch')\n","    for k, v in batch.items():\n","        if k=='idx':\n","            print(f'{k:>32} : {len(v)} ')\n","        else:\n","            print(f'{k:>32} : {v.shape} ')\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print(f'{k:>32} : {v.shape} ')\n","    print('loss')\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print(f'{k:>32} : {v.item()} ')\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["train = pl.read_parquet(os.path.join(\"../data/chuncked-dataset/\", \"local_train_enc_0.parquet\"), n_rows=None).to_pandas()"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["mask_df = (train.values > 0).astype(int)\n","mask_df = pd.DataFrame(mask_df, columns=train.columns)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>enc0</th>\n","      <th>enc1</th>\n","      <th>enc2</th>\n","      <th>enc3</th>\n","      <th>enc4</th>\n","      <th>enc5</th>\n","      <th>enc6</th>\n","      <th>enc7</th>\n","      <th>enc8</th>\n","      <th>enc9</th>\n","      <th>...</th>\n","      <th>enc135</th>\n","      <th>enc136</th>\n","      <th>enc137</th>\n","      <th>enc138</th>\n","      <th>enc139</th>\n","      <th>enc140</th>\n","      <th>enc141</th>\n","      <th>bind1</th>\n","      <th>bind2</th>\n","      <th>bind3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9743140</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9743141</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9743142</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9743143</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9743144</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9743145 rows × 145 columns</p>\n","</div>"],"text/plain":["         enc0  enc1  enc2  enc3  enc4  enc5  enc6  enc7  enc8  enc9  ...  \\\n","0           1     1     1     1     1     1     1     1     1     1  ...   \n","1           1     1     1     1     1     1     1     1     1     1  ...   \n","2           1     1     1     1     1     1     1     1     1     1  ...   \n","3           1     1     1     1     1     1     1     1     1     1  ...   \n","4           1     1     1     1     1     1     1     1     1     1  ...   \n","...       ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n","9743140     1     1     1     1     1     1     1     1     1     1  ...   \n","9743141     1     1     1     1     1     1     1     1     1     1  ...   \n","9743142     1     1     1     1     1     1     1     1     1     1  ...   \n","9743143     1     1     1     1     1     1     1     1     1     1  ...   \n","9743144     1     1     1     1     1     1     1     1     1     1  ...   \n","\n","         enc135  enc136  enc137  enc138  enc139  enc140  enc141  bind1  bind2  \\\n","0             0       0       0       0       0       0       0      0      0   \n","1             0       0       0       0       0       0       0      0      0   \n","2             0       0       0       0       0       0       0      0      0   \n","3             0       0       0       0       0       0       0      0      0   \n","4             0       0       0       0       0       0       0      0      0   \n","...         ...     ...     ...     ...     ...     ...     ...    ...    ...   \n","9743140       0       0       0       0       0       0       0      0      0   \n","9743141       0       0       0       0       0       0       0      0      0   \n","9743142       0       0       0       0       0       0       0      0      0   \n","9743143       0       0       0       0       0       0       0      0      0   \n","9743144       0       0       0       0       0       0       0      0      0   \n","\n","         bind3  \n","0            0  \n","1            0  \n","2            0  \n","3            0  \n","4            0  \n","...        ...  \n","9743140      0  \n","9743141      0  \n","9743142      0  \n","9743143      0  \n","9743144      0  \n","\n","[9743145 rows x 145 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["mask_df"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["train = pl.read_parquet(os.path.join(\"../data/chuncked-dataset/\", \"local_train_enc_0.parquet\"), n_rows=n_rows).to_pandas()\n","mask_df = pl.read_parquet(os.path.join(\"../data/chuncked-dataset/\", \"local_train_mask_0.parquet\"), n_rows=n_rows).to_pandas()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["FEATURES = [f'enc{i}' for i in range(140)]\n","TARGETS = ['bind1', 'bind2', 'bind3']\n","\n","\n","# バッチに分ける\n","def get_batch(df, mask_df, batch_size=32):\n","    for i in range(0, len(df), batch_size):\n","        batch = {}\n","        batch['smiles_token_id'] = torch.from_numpy(df[FEATURES].values[i:i+batch_size]).byte().cuda()\n","        batch['smiles_token_mask'] = torch.from_numpy(mask_df[FEATURES].values[i:i+batch_size]).byte().cuda()\n","        batch['bind'] = torch.from_numpy(df[TARGETS].values[i:i+batch_size]).float().cuda()\n","        yield batch\n","\n","batch = get_batch(train, mask_df, batch_size=4096)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch:0 iter:0 loss:0.6746259927749634\n","epoch:1 iter:0 loss:0.1499561071395874\n","epoch:2 iter:0 loss:0.19734856486320496\n","epoch:3 iter:0 loss:0.032289013266563416\n","epoch:4 iter:0 loss:0.04275503382086754\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iter:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_batch(train, mask_df, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)):\n\u001b[1;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n","Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(df, mask_df, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), batch_size):\n\u001b[1;32m      8\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmiles_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFEATURES\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyte\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmiles_token_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mask_df[FEATURES]\u001b[38;5;241m.\u001b[39mvalues[i:i\u001b[38;5;241m+\u001b[39mbatch_size])\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     11\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbind\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(df[TARGETS]\u001b[38;5;241m.\u001b[39mvalues[i:i\u001b[38;5;241m+\u001b[39mbatch_size])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","# 学習\n","def run_train():\n","    net = Net().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(30):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            with torch.cuda.amp.autocast(enabled=True):\n","                output = net(batch)\n","                \n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","    \n","run_train()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>32\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mcheck_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mcheck_net\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_net\u001b[39m():\n\u001b[1;32m      2\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m MAX_LENGTH\n\u001b[1;32m      5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmiles_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mtrain\u001b[49m[FEATURES]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmiles_token_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mask_df\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbind\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(train[TARGETS]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     11\u001b[0m     net \u001b[38;5;241m=\u001b[39m Net()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(net)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#net.train()\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}],"source":["def check_net():\n","    max_length = MAX_LENGTH\n","\n","\n","    batch = {\n","        'smiles_token_id': torch.from_numpy(train[FEATURES].values).byte().cuda(),\n","        'smiles_token_mask': torch.from_numpy(mask_df.values).byte().cuda(),\n","        'bind': torch.from_numpy(train[TARGETS].values).float().cuda(),\n","    }\n","     \n","    net = Net().cuda()\n","    #print(net)\n","    #net.train()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n","            output = net(batch)\n","\n","    # ---\n","    print('batch')\n","    for k, v in batch.items():\n","        if k=='idx':\n","            print(f'{k:>32} : {len(v)} ')\n","        else:\n","            print(f'{k:>32} : {v.shape} ')\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print(f'{k:>32} : {v.shape} ')\n","    print('loss')\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print(f'{k:>32} : {v.item()} ')\n","\n","\n","check_net()\n"]},{"cell_type":"markdown","metadata":{},"source":["I also tried other next generation seq models.  \n","e.g. mamba, xlstm, Griffin (deepmind's attention RNN)  \n","\n","I am looking for a faster alternative. But so far transformer + flash attnetion2 is still the fastest  (for small dim and num of layers). for performance, i think these models are smiliar."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:51.990281Z","iopub.status.busy":"2024-06-07T03:08:51.989893Z","iopub.status.idle":"2024-06-07T03:08:52.003495Z","shell.execute_reply":"2024-06-07T03:08:52.002567Z","shell.execute_reply.started":"2024-06-07T03:08:51.99025Z"},"trusted":true},"outputs":[],"source":["\n","\n","#xlstm model\n","# offical repo: https://github.com/NX-AI/xlstm\n","from xlstm import (\n","    xLSTMBlockStack,\n","    xLSTMBlockStackConfig,\n","    mLSTMBlockConfig,\n","    mLSTMLayerConfig,\n","    sLSTMBlockConfig,\n","    sLSTMLayerConfig,\n","    FeedForwardConfig,\n",")\n","\n","class Xlstm(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim = 256\n","\n","        self.output_type = ['infer', 'loss']\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=0)\n","\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3, stride=1, padding=1, is_bn=False),\n","        )\n","\n","        self.lstm_encoder = xLSTMBlockStack(\n","            config = xLSTMBlockStackConfig(\n","                mlstm_block=mLSTMBlockConfig(\n","                    mlstm=mLSTMLayerConfig(\n","                        conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n","                    )\n","                ),\n","                slstm_block=sLSTMBlockConfig(\n","                    slstm=sLSTMLayerConfig(\n","                        backend='cuda',\n","                        batch_size=64,\n","                        num_heads=8,\n","                        conv1d_kernel_size=4,\n","                        bias_init='powerlaw_blockdependent',\n","                    ),\n","                    feedforward=FeedForwardConfig(proj_factor=1.3, act_fn='gelu'),\n","                ),\n","                context_length=MAX_LENGTH,\n","                num_blocks=6,\n","                embedding_dim=embed_dim,\n","                slstm_at=[1],\n","            )\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","\n","    def forward(self, batch):\n","        smiles_token_id = batch['smiles_token_id'].long()\n","        B, L = smiles_token_id.shape\n","\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","\n","        x = self.lstm_encoder(x)\n","        last = x.mean(1)\n","        bind = self.bind(last)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["\n","# 学習\n","def run_train():\n","    net = Xlstm().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(10):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            output = net(batch)\n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","\n","run_train()"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-06-07T03:08:52.006055Z","iopub.status.busy":"2024-06-07T03:08:52.005646Z","iopub.status.idle":"2024-06-07T03:08:52.020395Z","shell.execute_reply":"2024-06-07T03:08:52.019248Z","shell.execute_reply.started":"2024-06-07T03:08:52.00601Z"},"trusted":true},"outputs":[],"source":["\n","# official repo: https://github.com/state-spaces/mamba\n","\n","#https://github.com/state-spaces/mamba/issues/355\n","# there is a bug? i cannot try mamba2\n","\n","from mamba_ssm import Mamba\n","from torch.nn.init import xavier_uniform_\n","from functools import partial\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\"TODO: mambaになっていない．線形層\"\n","class Mamba1(nn.Module):\n","    def __init__(self, d_model, d_intermediate, norm_epsilon, rms_norm, residual_in_fp32, fused_add_norm):\n","        super(Mamba1, self).__init__()\n","        self.d_model = d_model\n","        self.d_intermediate = d_intermediate\n","        self.norm_epsilon = norm_epsilon\n","        self.rms_norm = rms_norm\n","        self.residual_in_fp32 = residual_in_fp32\n","        self.fused_add_norm = fused_add_norm\n","\n","        # ネットワークレイヤーの定義\n","        self.linear1 = nn.Linear(d_model, d_intermediate)\n","        self.linear2 = nn.Linear(d_intermediate, d_model)\n","        self.norm = nn.LayerNorm(d_model, eps=norm_epsilon)\n","\n","    def forward(self, x, residual):\n","        if self.residual_in_fp32 and residual is not None:\n","            residual = residual.float()\n","\n","        x = self.linear1(x)\n","        x = F.relu(x)\n","        x = self.linear2(x)\n","\n","        if self.fused_add_norm:\n","            x = self.norm(x + residual) if residual is not None else self.norm(x)\n","        else:\n","            x = x + residual if residual is not None else x\n","            x = self.norm(x)\n","\n","        return x, residual\n","\n","def create_block(embed_dim, d_intermediate, ssm_cfg, attn_layer_idx, attn_cfg, norm_epsilon, rms_norm, residual_in_fp32, fused_add_norm, layer_idx):\n","    # ssm_cfg パラメータを解析して適切な層を選択\n","    if ssm_cfg['layer'] == 'Mamba1':\n","        return Mamba1(\n","            d_model=embed_dim,\n","            d_intermediate=d_intermediate,\n","            norm_epsilon=norm_epsilon,\n","            rms_norm=rms_norm,\n","            residual_in_fp32=residual_in_fp32,\n","            fused_add_norm=fused_add_norm\n","        )\n","    else:\n","        raise ValueError(\"Unsupported layer type specified in ssm_cfg\")\n","\n","# この create_block 関数は、与えられたパラメータに基づいて、適切な Mamba1 モジュールインスタンスを作成します。\n","\n","class Net(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim=256\n","        num_layer=6\n","        self.output_type = ['infer', 'loss']\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n","        self.pe = PositionalEncoding(embed_dim,max_len=256)\n","\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3,stride=1,padding=1, is_bn=True),\n","        )\n","\n","        self.mamba_encoder = nn.ModuleList(\n","            [\n","                create_block(\n","                    embed_dim,\n","                    d_intermediate=embed_dim//2,\n","                    ssm_cfg={'layer': 'Mamba1'},\n","                    attn_layer_idx=None,\n","                    attn_cfg=None,\n","                    norm_epsilon=1e-4,\n","                    rms_norm=1e-4,\n","                    residual_in_fp32=False,\n","                    fused_add_norm=True,\n","                    layer_idx=i,\n","                )\n","                for i in range(num_layer)\n","            ])\n","\n","        self.norm_f = nn.LayerNorm ( #RMSNorm\n","            embed_dim, eps=1e-4\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","        self.apply(\n","            partial(\n","                _init_weights,\n","                n_layer=num_layer,\n","                n_residuals_per_layer=2,\n","            )\n","        )\n","\n","    def forward(self, batch):\n","        smiles_token_id = batch['smiles_token_id'].long()\n","        smiles_token_mask = batch['smiles_token_mask'].long()\n","        B, L  = smiles_token_id.shape\n","\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","      \n","        hidden, residual = x, None\n","        for mamba in self.mamba_encoder:\n","            hidden, residual = mamba(\n","                hidden, residual\n","            )\n","            hidden = F.dropout(hidden,p=0.1, training=self.training)\n","\n","        #z=hidden\n","        if residual is not None:\n","            if residual_in_fp32:\n","                residual = residual.to(torch.float32)\n","            hidden += residual  # 残差接続\n","\n","        # LayerNorm を適用\n","        z = self.norm_f(hidden)\n","\n","        #pool = z.mean(1)\n","        m = smiles_token_mask.unsqueeze(2).float()\n","        pool = (z*m).sum(1)/m.sum(1)\n","        bind = self.bind(pool)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["batch\n","                 smiles_token_id : torch.Size([500, 160]) \n","               smiles_token_mask : torch.Size([500, 160]) \n","                            bind : torch.Size([500, 3]) \n","output\n","                            bind : torch.Size([500, 3]) \n","loss\n","                        bce_loss : 0.8870716094970703 \n"]}],"source":[" \n","#-------------------------------------\n","#dummy code to check net\n","def run_check_net():\n","    max_length = MAX_LENGTH\n","    batch_size = 500\n","\n","    batch = {\n","        'smiles_token_id': torch.from_numpy(np.random.choice(VOCAB_SIZE, (batch_size, max_length))).byte().cuda(),\n","        'smiles_token_mask': torch.from_numpy(np.random.choice(2, (batch_size, max_length))).byte().cuda(),\n","        'bind': torch.from_numpy(np.random.choice(2, (batch_size, 3))).float().cuda(),\n","    }\n","     \n","    net = Net().cuda()\n","    #print(net)\n","    #net.train()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n","            output = net(batch)\n","\n","    # ---\n","    print('batch')\n","    for k, v in batch.items():\n","        if k=='idx':\n","            print(f'{k:>32} : {len(v)} ')\n","        else:\n","            print(f'{k:>32} : {v.shape} ')\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print(f'{k:>32} : {v.shape} ')\n","    print('loss')\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print(f'{k:>32} : {v.item()} ')\n","\n","run_check_net()"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch:0 iter:0 loss:0.46796882152557373\n","epoch:1 iter:0 loss:0.03461839258670807\n","epoch:2 iter:0 loss:0.03337155282497406\n","epoch:3 iter:0 loss:0.03157204017043114\n","epoch:4 iter:0 loss:0.03174734115600586\n","epoch:5 iter:0 loss:0.03172867000102997\n","epoch:6 iter:0 loss:0.031632088124752045\n","epoch:7 iter:0 loss:0.031668342649936676\n","epoch:8 iter:0 loss:0.03160133212804794\n","epoch:9 iter:0 loss:0.031643111258745193\n"]}],"source":["# 学習\n","def run_train():\n","    net = Net().cuda()\n","    net.train()\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    for epoch in range(10):\n","        for i, batch in enumerate(get_batch(train, mask_df, batch_size=4096)):\n","            optimizer.zero_grad()\n","            output = net(batch)\n","            output['bce_loss'].backward()\n","            optimizer.step()\n","            if i%10==0:\n","                print(f'epoch:{epoch} iter:{i} loss:{output[\"bce_loss\"].item()}')\n","     \n","\n","run_train()"]},{"cell_type":"markdown","metadata":{},"source":["#### "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8006601,"sourceId":67356,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
