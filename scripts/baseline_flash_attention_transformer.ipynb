{"cells":[{"cell_type":"markdown","metadata":{},"source":["In this notebook we will train a deep learning model using all the data available !\n","* preprocessing : I encoded the smiles of all the train & test set and saved it [here](https://www.kaggle.com/datasets/ahmedelfazouan/belka-enc-dataset) , this may take up to 1 hour on TPU.\n","* Training & Inference : I used a simple 1dcnn model trained on 20 epochs.\n","\n","How to improve :\n","* Try a different architecture : I'm able to get an LB score of 0.604 with minor changes on this architecture.\n","* Try another model like Transformer, or LSTM.\n","* Train for more epochs.\n","* Add more features like a one hot encoding of bb2 or bb3.\n","* And of course ensembling with GBDT models."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gc\n","import os\n","import pickle\n","import random\n","import joblib\n","import pandas as pd\n","# import polars as pd\n","from tqdm import tqdm\n","from sklearn.metrics import average_precision_score as APS\n","import polars as pl\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","import torch.nn.functional as F\n","import math\n","\n","from module import network, dataset, util\n","from importlib import reload\n","\n","\n","from functools import partial\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T12:00:28.621503Z","iopub.status.busy":"2024-05-01T12:00:28.621165Z","iopub.status.idle":"2024-05-01T12:00:28.625731Z","shell.execute_reply":"2024-05-01T12:00:28.625098Z","shell.execute_reply.started":"2024-05-01T12:00:28.621462Z"},"trusted":true},"outputs":[],"source":["\n","\n","class Config:\n","    PREPROCESS = False\n","    KAGGLE_NOTEBOOK = False\n","    DEBUG = True\n","    \n","    SEED = 42\n","    EPOCHS = 1\n","    BATCH_SIZE = 4096\n","    LR = 1e-3\n","    WD = 1e-6\n","    PATIENCE = 10\n","    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    NBR_FOLDS = 15\n","    SELECTED_FOLDS = [0]\n","    EARLY_STOPPING = False\n","    \n","    \n","if Config.DEBUG:\n","    n_rows = 10**5\n","else:\n","    n_rows = None\n","    \n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if Config.KAGGLE_NOTEBOOK:\n","    RAW_DIR = \"/kaggle/input/leash-BELKA/\"\n","    PROCESSED_DIR = \"/kaggle/input/belka-enc-dataset\"\n","    OUTPUT_DIR = \"\"\n","    MODEL_DIR = \"\"\n","else:\n","    RAW_DIR = \"../data/raw/\"\n","    PROCESSED_DIR = \"../data/processed/\"\n","    OUTPUT_DIR = \"../data/result/\"\n","    MODEL_DIR = \"../models/\"\n","\n","TRAIN_DATA_NAME = \"train_enc.parquet\""]},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-01T12:00:28.626712Z","iopub.status.busy":"2024-05-01T12:00:28.626462Z","iopub.status.idle":"2024-05-01T12:01:07.833616Z","shell.execute_reply":"2024-05-01T12:01:07.832835Z","shell.execute_reply.started":"2024-05-01T12:00:28.626692Z"},"trusted":true},"outputs":[],"source":["def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","set_seeds(seed=Config.SEED)\n","\n","train_file_list = [f\"../data/chuncked-dataset/local_train_enc_{i}.parquet\" for i in range(10)]\n","mask_file_list = [f\"../data/chuncked-dataset/local_train_mask_{i}.parquet\" for i in range(10)]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","class FlashAttentionTransformerEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model,\n","        num_layers,\n","        num_heads=None,\n","        dim_feedforward=None,\n","        dropout=0.0,\n","        norm_first=False,\n","        activation=F.gelu,\n","        rotary_emb_dim=0,\n","    ):\n","        super().__init__()\n","\n","        try:\n","            from flash_attn.bert_padding import pad_input, unpad_input\n","            from flash_attn.modules.block import Block\n","            from flash_attn.modules.mha import MHA\n","            from flash_attn.modules.mlp import Mlp\n","        except ImportError:\n","            raise ImportError('Please install flash_attn from https://github.com/Dao-AILab/flash-attention')\n","        \n","        self._pad_input = pad_input\n","        self._unpad_input = unpad_input\n","\n","        if num_heads is None:\n","            num_heads = dim_model // 64\n","        \n","        if dim_feedforward is None:\n","            dim_feedforward = dim_model * 4\n","\n","        if isinstance(activation, str):\n","            activation = {\n","                'relu': F.relu,\n","                'gelu': F.gelu\n","            }.get(activation)\n","\n","            if activation is None:\n","                raise ValueError(f'Unknown activation {activation}')\n","\n","        mixer_cls = partial(\n","            MHA,\n","            num_heads=num_heads,\n","            use_flash_attn=True,\n","            rotary_emb_dim=rotary_emb_dim\n","        )\n","\n","        mlp_cls = partial(Mlp, hidden_features=dim_feedforward)\n","\n","        self.layers = nn.ModuleList([\n","            Block(\n","                dim_model,\n","                mixer_cls=mixer_cls,\n","                mlp_cls=mlp_cls,\n","                resid_dropout1=dropout,\n","                resid_dropout2=dropout,\n","                prenorm=norm_first,\n","            ) for _ in range(num_layers)\n","        ])\n","    \n","    def forward(self, x, src_key_padding_mask=None):\n","        batch, seqlen = x.shape[:2]\n","\n","        if src_key_padding_mask is None:\n","            for layer in self.layers:\n","                x = layer(x)\n","        else:\n","            x, indices, cu_seqlens, max_seqlen_in_batch = self._unpad_input(x, ~src_key_padding_mask)\n","            \n","            for layer in self.layers:\n","                x = layer(x, mixer_kwargs=dict(\n","                    cu_seqlens=cu_seqlens,\n","                    max_seqlen=max_seqlen_in_batch\n","                ))\n","      \n","\n","            x = self._pad_input(x, indices, batch, seqlen)\n","            \n","        return x\n","\n","class Conv1dBnRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, is_bn, **kwargs):\n","        super(Conv1dBnRelu, self).__init__()\n","        self.conv = nn.Conv1d(in_channels, out_channels, **kwargs)\n","        self.is_bn = is_bn\n","        if self.is_bn:\n","            self.bn = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.is_bn:\n","            x = self.bn(x)\n","        return self.relu(x)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=256):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[ :,:x.size(1)]\n","        \n","        return x\n","\n","class Net(nn.Module):\n","    def __init__(self, ):\n","        super().__init__()\n","\n","        embed_dim=512\n","\n","        self.output_type = ['infer', 'loss']\n","        self.pe = PositionalEncoding(embed_dim,max_len=256)\n","        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n","        self.conv_embedding = nn.Sequential(\n","            Conv1dBnRelu(64, embed_dim, kernel_size=3,stride=1,padding=1, is_bn=True),\n","        )  #just a simple conv1d-bn-relu . for bn use: BN = partial(nn.BatchNorm1d, eps=5e-3,momentum=0.1)\n","\n","        self.tx_encoder = FlashAttentionTransformerEncoder(\n","            dim_model=embed_dim,\n","            num_heads=8,\n","            dim_feedforward=embed_dim*4,\n","            dropout=0.1,\n","            norm_first=False,\n","            activation=F.gelu,\n","            rotary_emb_dim=0,\n","            num_layers=7,\n","        )\n","\n","        self.bind = nn.Sequential(\n","            nn.Linear(embed_dim, 3),\n","        )\n","\n","\n","    def forward(self, batch):\n","        smiles_token_id   = batch['smiles_token_id'].long()\n","        smiles_token_mask = batch['smiles_token_mask'].long()\n","        B, L = smiles_token_id.shape\n","        x = self.embedding(smiles_token_id)\n","        x = x.permute(0,2,1).float()\n","        x = self.conv_embedding(x)\n","        x = x.permute(0,2,1).contiguous()\n","\n","        x = self.pe(x)\n","        z = self.tx_encoder(\n","            x=x,\n","            src_key_padding_mask=smiles_token_mask==0,\n","        )\n","\n","\n","        m = smiles_token_mask.unsqueeze(-1).float()\n","        pool = (z*m).sum(1)/m.sum(1)\n","        bind = self.bind(pool)\n","\n","        # --------------------------\n","        output = {}\n","        if 'loss' in self.output_type:\n","            target = batch['bind']\n","            # pos_weight = torch.tensor([215, 241, 136], device=Config.DEVICE)\n","            pos_weight = torch.tensor([1, 1, 1], device=Config.DEVICE)\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float(), pos_weight=pos_weight)\n","\n","        if 'infer' in self.output_type:\n","            output['bind'] = torch.sigmoid(bind)\n","\n","        return output\n","\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, df, mask_df):\n","        self.df = df\n","        self.mask_df = mask_df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'smiles_token_id': torch.tensor(self.df.iloc[idx][FEATURES].values, dtype=torch.uint8),\n","            'smiles_token_mask': torch.tensor(self.mask_df.iloc[idx][FEATURES].values, dtype=torch.uint8),\n","            'bind': torch.tensor(self.df.iloc[idx][TARGETS].values, dtype=torch.float32)\n","        }\n","\n","class Trainer:\n","    def __init__(self, model, optimizer, device, patience, batch_size=4096):\n","        self.model = model.to(device)\n","        self.optimizer = optimizer\n","        self.device = device\n","        self.patience = patience\n","        self.batch_size = batch_size\n","\n","    def train_epoch(self, dataset):\n","        self.model.train()\n","        running_loss = 0.0\n","        total_samples = 0\n","        # num_workers=0だと遅い\n","        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=32, pin_memory=True)\n","        for batch in train_loader:\n","            batch = {k: v.to(self.device) for k, v in batch.items()}\n","            self.optimizer.zero_grad()\n","            with torch.cuda.amp.autocast(enabled=True):\n","                output = self.model(batch)\n","            loss = output['bce_loss']\n","            loss.backward()\n","            self.optimizer.step()\n","            batch_size = batch['smiles_token_id'].size(0)\n","            running_loss += loss.item() * batch_size\n","            total_samples += batch_size\n","\n","        epoch_loss = running_loss / total_samples\n","        return epoch_loss\n","\n","    def validate(self, dataset):\n","        self.model.eval()\n","        val_loss = 0.0\n","        total_samples = 0\n","\n","        \n","        val_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=32, pin_memory=True)\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                # cudaに乗せる\n","                batch = {k: v.to(self.device) for k, v in batch.items()}\n","                with torch.cuda.amp.autocast(enabled=True):\n","                    output = self.model(batch)\n","                loss = output['bce_loss']\n","                batch_size = batch['smiles_token_id'].size(0)\n","                val_loss += loss.item() * batch_size\n","                total_samples += batch_size\n","\n","        val_loss /= total_samples\n","        return val_loss\n","\n","    def train(self, train_datasets, mask_datasets, epochs):\n","        best_val_loss = float('inf')\n","        patience_counter = 0\n","        val_df = pl.read_parquet(train_file_list[9], n_rows=n_rows).to_pandas()\n","        val_mask_df = pl.read_parquet(mask_file_list[9], n_rows=n_rows).to_pandas()\n","        val_dataset = CustomDataset(val_df, val_mask_df)\n","        for epoch in range(epochs):\n","            train_df = pl.read_parquet(train_file_list[epoch % 9], n_rows=n_rows).to_pandas()\n","            mask_df = pl.read_parquet(mask_file_list[epoch % 9], n_rows=n_rows).to_pandas()\n","            train_dataset = CustomDataset(train_df, mask_df)\n","\n","            epoch_loss = self.train_epoch(train_dataset)\n","            val_loss = self.validate(val_dataset)\n","            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","            if Config.EARLY_STOPPING:\n","                if val_loss < best_val_loss:\n","                    best_val_loss = val_loss\n","                    torch.save(self.model.state_dict(), os.path.join(MODEL_DIR, 'best_model.pt'))\n","                    patience_counter = 0\n","                else:\n","                    patience_counter += 1\n","                    if patience_counter >= self.patience:\n","                        print('早期終了')\n","                        break\n","            else:\n","                torch.save(self.model.state_dict(), os.path.join(MODEL_DIR, 'best_model.pt'))\n","                print(\"model saved\")\n","\n","        return best_val_loss\n","    \n","\n","def predict_in_batches(model, val_df, val_mask_df, batch_size=4096):\n","    model.eval()\n","    preds = []\n","    dataset = CustomDataset(val_df, val_mask_df)\n","    val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=True)\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}\n","            with torch.cuda.amp.autocast(enabled=True):\n","                output = model(batch)\n","            output = output[\"bind\"]\n","            preds.append(output)\n","    preds = torch.cat(preds, dim=0).cpu().numpy()\n","    return preds\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1, Train Loss: 0.3269, Val Loss: 0.0501\n","model saved\n","[[0.02457 0.02414 0.04492]\n"," [0.02457 0.02419 0.04486]\n"," [0.02457 0.02428 0.04486]\n"," ...\n"," [0.02466 0.02414 0.045  ]\n"," [0.02457 0.02414 0.04492]\n"," [0.02461 0.02419 0.04492]]\n","Val Score: 0.0091\n","[[0.02461 0.0241  0.04492]\n"," [0.02457 0.02419 0.04486]\n"," [0.02461 0.02414 0.04486]\n"," ...\n"," [0.02457 0.02419 0.04486]\n"," [0.02457 0.02423 0.04477]\n"," [0.02466 0.0241  0.04492]]\n","train Score: 0.0084\n"]}],"source":["\n","# 定数やモデルの定義は適宜修正してください\n","FEATURES = [f'enc{i}' for i in range(142)]\n","TARGETS = ['bind1', 'bind2', 'bind3']\n","\n","\n","# https://www.ascii-code.com/\n","MOLECULE_DICT = {\n","    'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n","    '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25,\n","    '=': 26, '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36\n","}\n","MAX_MOLECULE_ID = np.max(list(MOLECULE_DICT.values()))\n","VOCAB_SIZE = MAX_MOLECULE_ID + 10\n","UNK = 255  # disallow: will cuase error\n","BOS = MAX_MOLECULE_ID + 1\n","EOS = MAX_MOLECULE_ID + 2\n","# rest are reserved\n","PAD = 0\n","MAX_LENGTH = 160\n","model = Net().to(Config.DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=Config.LR, weight_decay=Config.WD)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, min_lr=1e-6)\n","\n","# データの準備\n","trainer = Trainer(model, optimizer, Config.DEVICE, Config.PATIENCE)\n","trainer.train(train_file_list, mask_file_list, Config.EPOCHS)\n","\n","# 最良のモデルをロードして予測を行う\n","model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'best_model.pt')))\n","\n","\n","val_df = pl.read_parquet(train_file_list[9], n_rows=n_rows).to_pandas()\n","val_mask_df = pl.read_parquet(mask_file_list[9], n_rows=n_rows).to_pandas()\n","\n","preds = predict_in_batches(model, val_df, val_mask_df)\n","print(preds)\n","val_score = util.get_score(val_df[TARGETS].values, preds)\n","print(f'Val Score: {val_score:.4f}')\n","\n","\n","train_df = pl.read_parquet(train_file_list[0], n_rows=n_rows).to_pandas()\n","train_mask_df = pl.read_parquet(mask_file_list[0], n_rows=n_rows).to_pandas()\n","preds = predict_in_batches(model, train_df, train_mask_df)\n","print(preds)\n","train_score = util.get_score(train_df[TARGETS].values, preds)\n","print(f'train Score: {train_score:.4f}')\n","\n","# GPU使用率が上がらない問題は，データロードがボトルネックだった．numworkerとpin_memoryを使うことで解決した．\n","# 10^5\n","# numworkers=0 4m\n","# numworkers=16 44s\n","# numworkers=32 48s\n","# numworkers=32 pin memory 43s\n","# Epoch 1/1, Train Loss: 0.3269, Val Loss: 0.0501\n","\n","# pos weight\n","# Epoch 1/1, Train Loss: 2.9652, Val Loss: 2.4232\n","\n","# 10^6\n","# numworkers=32 pin memory 3m40s\n","\n","# None\n","# numworkers=32 pin memory 30m\n","# Epoch 1/1, Train Loss: 0.0385, Val Loss: 0.0336\n","\n","# pos weight\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.status.busy":"2024-05-01T12:07:11.898726Z","iopub.status.idle":"2024-05-01T12:07:11.899062Z","shell.execute_reply":"2024-05-01T12:07:11.89893Z","shell.execute_reply.started":"2024-05-01T12:07:11.898916Z"},"trusted":true},"outputs":[{"ename":"IndexError","evalue":"boolean index did not match indexed array along dimension 0; dimension is 100000 but corresponding boolean dimension is 1674896","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m mask_sEH \u001b[38;5;241m=\u001b[39m (tst[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msEH\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 各マスクに対応する予測値を代入\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m tst\u001b[38;5;241m.\u001b[39mloc[mask_BRD4, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_BRD4\u001b[49m\u001b[43m]\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m tst\u001b[38;5;241m.\u001b[39mloc[mask_HSA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[mask_HSA][:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     15\u001b[0m tst\u001b[38;5;241m.\u001b[39mloc[mask_sEH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[mask_sEH][:, \u001b[38;5;241m2\u001b[39m]\n","\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 100000 but corresponding boolean dimension is 1674896"]}],"source":["\n","# テストデータの読み込み\n","tst = pl.read_parquet(os.path.join(RAW_DIR, \"test.parquet\"), n_rows=None).to_pandas()\n","\n","# 'binds'列を追加して初期化\n","tst['binds'] = 0\n","\n","# ブールマスクの作成\n","mask_BRD4 = (tst['protein_name'] == 'BRD4').values\n","mask_HSA = (tst['protein_name'] == 'HSA').values\n","mask_sEH = (tst['protein_name'] == 'sEH').values\n","\n","# 各マスクに対応する予測値を代入\n","tst.loc[mask_BRD4, 'binds'] = preds[mask_BRD4][:, 0]\n","tst.loc[mask_HSA, 'binds'] = preds[mask_HSA][:, 1]\n","tst.loc[mask_sEH, 'binds'] = preds[mask_sEH][:, 2]\n","\n","\n","\n","submission = tst[['id', 'binds']].copy()\n","# 'id'と'binds'列をCSVに出力\n","submission.to_csv(os.path.join(OUTPUT_DIR,'submission.csv'), index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>binds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>295246830</td>\n","      <td>0.507840</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>295246831</td>\n","      <td>0.490291</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>295246832</td>\n","      <td>0.284896</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>295246833</td>\n","      <td>0.516321</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>295246834</td>\n","      <td>0.482399</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1674891</th>\n","      <td>296921721</td>\n","      <td>0.393290</td>\n","    </tr>\n","    <tr>\n","      <th>1674892</th>\n","      <td>296921722</td>\n","      <td>0.377067</td>\n","    </tr>\n","    <tr>\n","      <th>1674893</th>\n","      <td>296921723</td>\n","      <td>0.465928</td>\n","    </tr>\n","    <tr>\n","      <th>1674894</th>\n","      <td>296921724</td>\n","      <td>0.552328</td>\n","    </tr>\n","    <tr>\n","      <th>1674895</th>\n","      <td>296921725</td>\n","      <td>0.468154</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1674896 rows × 2 columns</p>\n","</div>"],"text/plain":["                id     binds\n","0        295246830  0.507840\n","1        295246831  0.490291\n","2        295246832  0.284896\n","3        295246833  0.516321\n","4        295246834  0.482399\n","...            ...       ...\n","1674891  296921721  0.393290\n","1674892  296921722  0.377067\n","1674893  296921723  0.465928\n","1674894  296921724  0.552328\n","1674895  296921725  0.468154\n","\n","[1674896 rows x 2 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["submission"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"databundleVersionId":8006601,"sourceId":67356,"sourceType":"competition"},{"datasetId":4914065,"sourceId":8275617,"sourceType":"datasetVersion"}],"dockerImageVersionId":30514,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
